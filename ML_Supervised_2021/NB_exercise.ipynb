{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq_3t_tluNmU"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQVMUZkQHlm2"
   },
   "source": [
    "## 1. Classifying Digits\n",
    "In this part we will test digits classification on the MNIST dataset, using Bernoulli Naive Bayes (a generative model).\n",
    "\n",
    "The MNIST dataset contains 28x28 grayscale images of handwritten digits between 0 and 9 (10 classes). For mathmatical analysis clarity, and for matching expected API, flatten each image to create a 1D array with 784 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjwjk6pzLE-y"
   },
   "source": [
    "### Loading the MNIST dataset\n",
    "Load the MNIST data set. The digits dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. Use \n",
    ">```\n",
    "mnist = sklearn.datasets.fetch_mldata('MNIST original')\n",
    "```\n",
    "\n",
    "to fetch the original data. You may set the `data_home` to where you wish to download your data for caching. Each image is already transformed into a 1D integer array $x\\in [0,255]^{784}$, and the corresponding label is an integer $y\\in [0,9]$.\n",
    "\n",
    "Plot a single sample of each digit as the original image, so you get a feeling how the data looks like.\n",
    "\n",
    "Finally, divide your data into train and test sets, using 1/7 of the data for testing.\n",
    "\n",
    "---\n",
    "**Note 1:** Using `digits = sklearn.datasets.load_digits()` will only fetch a very small sample of the original set, with images resized to 8x8. This preprocessing of the data reduces dimensionality and gives invariance to small distortions - however, we will use the original data in this exercise. Feel free to test the proformance of the algorithms below on the preprocessed data as well.\n",
    "\n",
    "**Note 2:**\n",
    "Since ML-Data is deprecated, you may wish to use something like this:\n",
    ">```\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T08:21:00.456579Z",
     "start_time": "2021-12-09T08:21:00.235092Z"
    },
    "id": "05Om0QLxBvvT"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "from keras.datasets import mnist\n",
    "(x_train_orig, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train_orig.reshape(x_train_orig.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T12:07:26.944599Z",
     "start_time": "2021-12-09T12:07:26.939612Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTGjjSKaFfE6"
   },
   "source": [
    "### Bernoulli Naive Bayes\n",
    "If we know how the digits are generated, then we know how to classify them (simply choose the digit class which will maximize the posterior probability) --- but which model should we use for describing the digits generation?\n",
    "\n",
    "In this part we will try a very simplified model of digits creation (which is obviously not the same as the \"real\" model), using a Naive Bayes over an underlying Bernoulli distribution --- that is, we will assume that given a digit class, the pixels of the images are the result of independent coin flips, each with its own \"head\" probability.\n",
    "\n",
    "Note that since we assume each pixl is either 0 (black) or 1 (white), we will need to adjust (preprocess) our data accrodingly (see below).\n",
    "\n",
    "So, the model is stated as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Domain} && x \\in \\{0,1\\}^{784} \\\\\n",
    "\\text{Prior} && \\pi_j = \\Pr(y=j) \\\\\n",
    "\\text{Likelihood} && P_j(x) = \\Pr(x | y=j) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where for each $i\\in 0\\ldots 784$ it holds that\n",
    "$$\n",
    "P_{ji}(x_i) = \\Pr(x_i | y=j) =\n",
    "\\begin{cases}\n",
    "p_{ji} & \\text{if } x_i=1 \\\\\n",
    "1-p_{ji} & \\text{if } x_i=0 \\\\\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNjhD3IpL5bC"
   },
   "source": [
    "#### Question 1\n",
    "Write the classification rule based on this Naive Bayes model. \n",
    "How would you esitmate each of the parameters of the model based on the trainning data? \n",
    "\n",
    "\n",
    "**Bonus:** Think of edge cases which may effect your estimator in an undesired way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pN1prGcMqwZ"
   },
   "source": [
    "#### Answer 1\n",
    "Assuming all $\\pi_{j}$ are equal i.e. uniform prior, along with indepence of each pixel, the classification rule is:\n",
    "> $argmax_{j}$ $\\Pi_{i}$($x_{ij}$*$p_{ij}$)+(1-$x_{ij}$)(1-$p_{ij}$)\n",
    "\n",
    "\n",
    "The estimate of $p_{ij}$ is the mean of $x_{i}$ for each $j$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOnkgDIXTMCQ"
   },
   "source": [
    "#### Question 2\n",
    "Run a Naive Bayes classifier on the training data and apply predictions on the test data. Use the [sklearn.naive_bayes.BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) implementation (see the [source code for sklearn.naive_bayes](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py) for details).\n",
    "\n",
    "Remember we need to preprocess the data in this case such that each pixel would become either black (0) or white (1). For this, use the `binarize` parameter of the implementation. Set this value to $0$ (this is the default), which in this case would mean every pixel with non-zero value will be set to 1.\n",
    "\n",
    "1. Plot the mean image of each class (estimated $\\hat{p}_{ji}$) and generate one sample of each class (remember, you can do this since this is a generative model). You will need to access the `feature_log_prob_` attribute of the trained model.\n",
    "\n",
    "2. Plot the confusion matrix of your classifier, as claculated on the test data (it is recommended to use [sklearn.metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)). Calculate the total accuracy (fraction of correctly classified images), and summarize the results in your own words.\n",
    "\n",
    "3. Think of a way you can find the optimal threshold of the binarization part. **There is no need to actually perform this task --- just describe what you would have done.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKdsdegDWaO_"
   },
   "source": [
    "#### Answer 2\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T12:45:13.965704Z",
     "start_time": "2021-12-09T12:45:13.952739Z"
    },
    "id": "XalqjRWXWS-Y"
   },
   "outputs": [],
   "source": [
    "# code goes here\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T12:47:34.661780Z",
     "start_time": "2021-12-09T12:47:34.559840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T12:48:30.517843Z",
     "start_time": "2021-12-09T12:48:30.509897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_log_prob_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:13:20.608641Z",
     "start_time": "2021-12-09T13:11:42.551758Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:15:47.506903Z",
     "start_time": "2021-12-09T13:15:47.493938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:22:35.499324Z",
     "start_time": "2021-12-09T13:22:35.484409Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({'target':data['target'],'data':data['data']}).to_pickle('sup_data')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV=CountVectorizer(stop_words='english')\n",
    "X=CV.fit_transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:46:00.079326Z",
     "start_time": "2021-12-09T13:46:00.064368Z"
    }
   },
   "outputs": [],
   "source": [
    "y=pd.get_dummies(data.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:48:18.347293Z",
     "start_time": "2021-12-09T13:48:18.317645Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import safe_sparse_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:48:43.105700Z",
     "start_time": "2021-12-09T13:48:43.098682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 129796), (11314, 20))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:49:57.349036Z",
     "start_time": "2021-12-09T13:49:57.335073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:48:58.293716Z",
     "start_time": "2021-12-09T13:48:58.226850Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-2dcf3a885c16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_frame_arith_method_with_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[1;34m(left, right, axis)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;31m# GH17901\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m                 raise ValueError(\n\u001b[0;32m    639\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[0;32m    296\u001b[0m                         \" or shape[0]\")\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "safe_sparse_dot(y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEbVc4Ixxe6L"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "  def predict_log_proba(self, x):\n",
    "    pass  \n",
    "  def predict(self, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKSgnectrTJ1"
   },
   "source": [
    "## 2. Classifing Text Documents using Multinomial Naive Bayes\n",
    "In this exercise you will classify the \"20 newsgroups\" data set using your own naive bayes classifier and compare to the scikit learn built in version.\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon messages posted before and after a specific date.\n",
    "\n",
    "\n",
    "* Load the **train** data using `from sklearn.datasets import fetch_20newsgroups`. remove headers, footers and quotes (see documentation)\n",
    "* Use `sklearn.feature_extraction.text import CountVectorizer` to count words (stop_words='english')\n",
    "* Write a class `NaiveBayes(BaseEstimator, ClassifierMixin)` and implement its `fit`, `predict` and `predict_proba` methods.\n",
    "* use `sklearn.pipeline.make_pipeline` to chain the vectroizer and model.\n",
    "* note: limit the vocuabolary size if you suffer memory issues\n",
    "* compare the accuracy over the **test** data. You can use `accuracy_score, classification_report`\n",
    "* compare to the built in `sklearn.naive_bayes.MultinomialNB`. If there are differences try to think why\n",
    "* plot the learning curve - is the model in the bias or variance regime (you can use the built in model for doing the analysis)\n",
    "* optimize performance in respect to vectorizer hyper parameters (e.g. max_features, max_df etc.).\n",
    "\n",
    "### Optional: Model interpretability\n",
    "Find the most important features for a **specific** decision of a NB classifier.\n",
    "Because the model has learned the prior $p(x_i|c)$ during the training, the contribution of an individual feature value can be easily measured by the posterior, $p(c|x_i)=p(c)p(x_i|c)/p(x_i)$\n",
    "Implement a function which gets a scikit-learn NB model as input and returns $P(c|x_i)$:\n",
    "\n",
    "`def calc_p_c_given_xi(model)`\n",
    "\n",
    "Hint: Use the following model properties:\n",
    "\n",
    "* `model.class_log_prior_`\n",
    "* `model.feature_log_prob_`\n",
    "\n",
    "Note: remember these are logs and you need to use np.exp and normalize to get $P(c|x_i)$ \n",
    "Another hint: use numpy built-in broadcasting property.\n",
    "\n",
    "* Use the interpretation to examine errors of the classifier where $\\hat{c}\\ne c$. Which top words support the correct class and which support the wrong class? You can use the `print_txt` below to color words. \n",
    "\n",
    "Bonus: How can you correct the analyzed error? \n",
    "\n",
    "To read more about model interpretation, see the blogpost below and my tutorial:\n",
    "* https://lilianweng.github.io/lil-log/2017/08/01/how-to-explain-the-prediction-of-a-machine-learning-model.html\n",
    "* https://github.com/chanansh/right_but_why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WrTOhkHV9msW",
    "outputId": "c69cb41c-5ed6-4e43-ef3f-6ccaa4980c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This \u001b[42;37mword\u001b[0m support the first class but this the \u001b[41;37mother\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def print_txt(txt, hot, cold):\n",
    "  \"\"\"\n",
    "  print the text, coloring hot and cold words with colors\n",
    "  \"\"\"\n",
    "  cold_color='\\x1b[41;37m{}\\x1b[0m'\n",
    "  hot_color='\\x1b[42;37m{}\\x1b[0m'\n",
    "  def color(token):\n",
    "    lower = str(token).lower()\n",
    "    lower = lower.replace('\\t','').replace('\\n','')\n",
    "    lower = lower.translate(string.punctuation)\n",
    "    if (lower in hot) and (lower in cold):\n",
    "      return mid_color.format(token)\n",
    "    elif lower in hot:\n",
    "      return hot_color.format(token)\n",
    "    elif lower in cold:\n",
    "      return cold_color.format(token)\n",
    "    else:\n",
    "      return token\n",
    "  colored_txt = \" \".join([color(token) for token in txt.split(' ')])\n",
    "  print(colored_txt)\n",
    "print_txt('This word support the first class but this the other', ['word'],['other'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NB exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
